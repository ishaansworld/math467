# math467
Theory and Computational Methods for Optimization

## Project 1

In this project, I manually updated neural network weights and built a loss function. The goal was to get an introductory understanding of how training works.

## Project 2

This project was an extension of project 1, where there was an emphasis on training for accuracy using 3 methods: Fixed Step Size, Steepest Descent, and Stochastic Gradient Descent (SGD). I used all 3 methods for the purposes of training one network to mimic results of another, and I compared success in minimizing the difference between the 2 networks' weights.
